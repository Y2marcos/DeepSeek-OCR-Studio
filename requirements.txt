torch==2.6.0
transformers==4.46.3
tokenizers==0.20.3
accelerate
einops
addict 
easydict
torchvision
# flash-attn (Linux GPU wheel) removed because it's a platform-specific wheel that fails on Windows.
# If you need flash-attn on Linux with a matching CUDA/PyTorch, install it manually per your platform:
#   pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
PyMuPDF
hf_transfer
gradio
Pillow
numpy
spaces